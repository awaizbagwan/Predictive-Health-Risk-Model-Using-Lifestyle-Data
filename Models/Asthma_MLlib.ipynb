{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzXT2iadAOxN",
        "outputId": "b8e7bcc0-30ca-4d6e-a514-395a14543730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=cd8fe011f7040991b7699ccee212a23c9b50387e5c19f18680a9248cee39ee15\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"RandomForestClassification\").getOrCreate()\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "df = spark.read.csv('/content/final_encoded_disease_csv.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Identify string columns and index them\n",
        "string_columns = [col for col, dtype in df.dtypes if dtype == 'string' and col != 'HadAsthmaIndexed']\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"Indexed\") for col in string_columns]\n",
        "\n",
        "# Apply StringIndexers to convert string columns to numerical indices\n",
        "for indexer in indexers:\n",
        "    df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Include indexed string columns in the features list\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[col for col in df.columns if (col != 'HadAsthmaIndexed' and col not in string_columns) or col.endswith(\"Indexed\")],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(labelCol=\"HadAsthmaIndexed\", featuresCol=\"features\", numTrees=100, seed=42)\n",
        "\n",
        "# Set up the cross-validation process\n",
        "paramGrid = ParamGridBuilder().build()  # No hyperparameters to tune in this example\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"HadAsthmaIndexed\", metricName=\"accuracy\")\n",
        "\n",
        "crossval = CrossValidator(estimator=rf,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)\n",
        "\n",
        "# Train the model\n",
        "cvModel = crossval.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = cvModel.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"HadAsthmaIndexed\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "\n",
        "# Print confusion matrix\n",
        "predictions.groupBy(\"HadAsthmaIndexed\", \"prediction\").count().show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkx8CIueASEK",
        "outputId": "d2ad58b2-6db9-41c8-8802-a7c8a9fee6f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "F1 Score: 1.00\n",
            "+----------------+----------+-----+\n",
            "|HadAsthmaIndexed|prediction|count|\n",
            "+----------------+----------+-----+\n",
            "|               0|       0.0|61089|\n",
            "|               1|       1.0|10474|\n",
            "+----------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"XGBoostClassification\").getOrCreate()\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "df = pd.read_csv('/content/final_encoded_disease_csv.csv')  # Pandas DataFrame\n",
        "sdf = spark.createDataFrame(df)  # Convert Pandas DataFrame to Spark DataFrame\n",
        "\n",
        "# Convert string columns to categorical using StringIndexer\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(sdf) for col in ['SmokerStatusVec', 'AgeCategoryVec']]\n",
        "for indexer in indexers:\n",
        "    sdf = indexer.transform(sdf).drop(indexer.getInputCol()).withColumnRenamed(indexer.getOutputCol(), indexer.getInputCol())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[col for col in sdf.columns if col != 'HadAsthmaIndexed'],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "sdf = assembler.transform(sdf).select(\"features\", \"HadAsthmaIndexed\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = sdf.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame for XGBoost\n",
        "train_pd = train_df.toPandas()\n",
        "test_pd = test_df.toPandas()\n",
        "\n",
        "# Extract features and target, and convert features to a NumPy array for XGBoost\n",
        "X_train = train_pd.drop(columns=[\"HadAsthmaIndexed\"])['features'].apply(lambda x: x.toArray()).tolist() # Convert features to list of arrays\n",
        "y_train = train_pd[\"HadAsthmaIndexed\"]\n",
        "\n",
        "X_test = test_pd.drop(columns=[\"HadAsthmaIndexed\"])['features'].apply(lambda x: x.toArray()).tolist() # Convert features to list of arrays\n",
        "y_test = test_pd[\"HadAsthmaIndexed\"]\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False,\n",
        "                          eval_metric='logloss',\n",
        "                          random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP5OF2ZqBG7a",
        "outputId": "461bb727-0965-462e-b8fa-421aa5553cce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:49:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.99      0.92     61141\n",
            "           1       0.53      0.10      0.17     10431\n",
            "\n",
            "    accuracy                           0.86     71572\n",
            "   macro avg       0.70      0.54      0.55     71572\n",
            "weighted avg       0.82      0.86      0.81     71572\n",
            "\n",
            "Confusion Matrix:\n",
            "[[60228   913]\n",
            " [ 9385  1046]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"SVMClassification\").getOrCreate()\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "df = spark.read.csv('/content/final_encoded_disease_csv.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Identify string columns and index them\n",
        "string_columns = [col for col, dtype in df.dtypes if dtype == 'string' and col != 'HadAsthmaIndexed']\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"Indexed\") for col in string_columns]\n",
        "\n",
        "# Apply StringIndexers to convert string columns to numerical indices\n",
        "for indexer in indexers:\n",
        "    df = indexer.fit(df).transform(df)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[col for col in df.columns if (col != 'HadAsthmaIndexed' and col not in string_columns) or col.endswith(\"Indexed\")],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Initialize the LinearSVC (SVM) classifier\n",
        "svm = LinearSVC(labelCol=\"HadAsthmaIndexed\", featuresCol=\"features\", maxIter=100, regParam=0.1)\n",
        "\n",
        "# Set up the cross-validation process\n",
        "paramGrid = ParamGridBuilder().build()  # No hyperparameters to tune in this example\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"HadAsthmaIndexed\", metricName=\"accuracy\")\n",
        "\n",
        "crossval = CrossValidator(estimator=svm,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)\n",
        "\n",
        "# Train the model\n",
        "cvModel = crossval.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = cvModel.transform(test_df)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"HadAsthmaIndexed\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "\n",
        "# Print confusion matrix\n",
        "predictions.groupBy(\"HadAsthmaIndexed\", \"prediction\").count().show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy8ApZZYRODe",
        "outputId": "6d8b98fc-e3bb-4991-aa78-61e39e99175c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "F1 Score: 1.00\n",
            "+----------------+----------+-----+\n",
            "|HadAsthmaIndexed|prediction|count|\n",
            "+----------------+----------+-----+\n",
            "|               0|       0.0|61089|\n",
            "|               1|       1.0|10474|\n",
            "+----------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"LogisticRegressionClassification\").getOrCreate()\n",
        "\n",
        "# Load your dataset into a DataFrame\n",
        "df = pd.read_csv('/content/final_encoded_disease_csv.csv')  # Pandas DataFrame\n",
        "sdf = spark.createDataFrame(df)  # Convert Pandas DataFrame to Spark DataFrame\n",
        "\n",
        "# Identify columns with non-numeric values and handle them\n",
        "non_numeric_columns = [col for col, dtype in sdf.dtypes if dtype == 'string']\n",
        "\n",
        "# Convert categorical features to numerical using StringIndexer\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in non_numeric_columns]\n",
        "for indexer in indexers:\n",
        "    sdf = indexer.fit(sdf).transform(sdf)\n",
        "    sdf = sdf.drop(indexer.getInputCol()).withColumnRenamed(indexer.getOutputCol(), indexer.getInputCol())\n",
        "\n",
        "# Fill any missing values with 0 (or use another appropriate strategy)\n",
        "sdf = sdf.na.fill(0)\n",
        "\n",
        "# Assemble feature columns into a single vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[col for col in sdf.columns if col != 'HadAsthmaIndexed'],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "sdf = assembler.transform(sdf).select(\"features\", \"HadAsthmaIndexed\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_df, test_df = sdf.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression(featuresCol='features', labelCol='HadAsthmaIndexed', maxIter=1000)\n",
        "\n",
        "# Train the model\n",
        "log_reg_model = log_reg.fit(train_df)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = log_reg_model.transform(test_df)\n",
        "\n",
        "# Evaluate the model using accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"HadAsthmaIndexed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"HadAsthmaIndexed\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "\n",
        "# Print confusion matrix\n",
        "predictions.groupBy('HadAsthmaIndexed', 'prediction').count().show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tizQ8SHBJC4",
        "outputId": "ef0677c8-3f34-4f09-ec68-549b748ae307"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "F1 Score: 0.81\n",
            "+----------------+----------+-----+\n",
            "|HadAsthmaIndexed|prediction|count|\n",
            "+----------------+----------+-----+\n",
            "|               1|       0.0| 9700|\n",
            "|               0|       1.0|  558|\n",
            "|               0|       0.0|60583|\n",
            "|               1|       1.0|  731|\n",
            "+----------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}